{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121\n!pip install unsloth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T01:54:33.549411Z","iopub.execute_input":"2024-11-17T01:54:33.549717Z","iopub.status.idle":"2024-11-17T01:58:51.816742Z","shell.execute_reply.started":"2024-11-17T01:54:33.549685Z","shell.execute_reply":"2024-11-17T01:58:51.815491Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\nimport re\nimport pandas as pd\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported\nimport random\n\n# Step 1: Load Llama-3, 8B Model\nmax_seq_length = 2048  \nload_in_4bit = True \ndtype = None  \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/Meta-Llama-3.1-8B\",\n    max_seq_length=max_seq_length,\n    dtype=dtype,\n    load_in_4bit=load_in_4bit,\n)\n\n# Adjust LoRA Parameters and Quantization strategies for better model efficiency and performance\nmodel = FastLanguageModel.get_peft_model(\n    model,\n    r=32,  # Adjusted for LoRA configuration, allowing for richer fine-tuning\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=32,  # Adjust lora_alpha for optimal performance\n    lora_dropout=0.1,  # Introduce dropout to reduce overfitting\n    bias=\"none\",\n    use_gradient_checkpointing=\"unsloth\",\n    random_state=3407,\n    use_rslora=True,  # Enable rank-stabilized LoRA\n    loftq_config=None\n)\n\n# Save the model after adjustment to avoid re-running the above process in case of errors\nmodel.save_pretrained('outputs_adjusted')\ntokenizer.save_pretrained('outputs_adjusted')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T01:59:53.905385Z","iopub.execute_input":"2024-11-17T01:59:53.905845Z","iopub.status.idle":"2024-11-17T02:00:59.540722Z","shell.execute_reply.started":"2024-11-17T01:59:53.905803Z","shell.execute_reply":"2024-11-17T02:00:59.539677Z"}},"outputs":[{"name":"stdout","text":"ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n==((====))==  Unsloth 2024.11.7: Fast Llama patching. Transformers = 4.46.2.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform = Linux.\nO^O/ \\_/ \\    Pytorch: 2.5.1+cu121. CUDA = 7.5. CUDA Toolkit = 12.1.\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.28.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079d34b89efe471a8eeb3ddcb929b9fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0da847adc4b43348921c49dd1933717"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60b15309c67142939bc4139a0d579c65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8fd32a7e9e894935af98316a7ae83f59"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c4c3883df64e148e3a95d7cf59f540"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.1.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2024.11.7 patched 32 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"('outputs_adjusted/tokenizer_config.json',\n 'outputs_adjusted/special_tokens_map.json',\n 'outputs_adjusted/tokenizer.json')"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Step 2: Load and Preprocess Dataset\ndataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n\n# Randomly sample 30K examples from the training dataset\ntrain_dataset = dataset['train'].shuffle(seed=42).select(range(30000))  # Select 30K samples for training\n\n# Use a prompt to Improve Model Understanding\nprompt = \"\"\"You are a great mathematician and your task is to determine if the answer to a given math question is correct or not. First, analyze the solution carefully, then decide if the answer is 'True' if correct, otherwise 'False'. Below is the Question, Answer, and Solution.\n\n### Question:\n{}\n\n### Answer:\n{}\n\n### Solution:\n{}\n\n### Analysis:\nBased on the solution provided above, your output should be:\n\n### Output:\n{}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token  # Add EOS token to ensure clear end of generated sequences\n\ndef formatting_prompts_func(examples):\n    questions = examples[\"question\"]\n    answers = examples[\"answer\"]\n    solutions = examples[\"solution\"]\n    outputs = examples[\"is_correct\"]\n    texts = []\n    for question, answer, solution, output in zip(questions, answers, solutions, outputs):\n        text = prompt.format(question, answer, solution, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Apply prompt formatting to the training data to make use of the 'Solution' column\ntrain_dataset = train_dataset.map(formatting_prompts_func, batched=True)\n\n# Save the dataset after preprocessing to avoid repeating if errors occur\ntrain_dataset.save_to_disk('train_dataset_preprocessed')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T02:01:53.333123Z","iopub.execute_input":"2024-11-17T02:01:53.333519Z","iopub.status.idle":"2024-11-17T02:01:55.847273Z","shell.execute_reply.started":"2024-11-17T02:01:53.333482Z","shell.execute_reply":"2024-11-17T02:01:55.846240Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac9104d8d66f4ec583a98c783c4755d1"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# Step 3: Set Training Arguments with Hyperparameter Tuning\ntraining_args = TrainingArguments(\n    per_device_train_batch_size=4,  \n    gradient_accumulation_steps=2,  \n    warmup_steps=50,  \n    max_steps=1000,  \n    learning_rate=2e-5,  \n    fp16=not is_bfloat16_supported(),\n    bf16=is_bfloat16_supported(),\n    logging_steps=20, \n    optim=\"adamw_8bit\",\n    weight_decay=0.05, \n    lr_scheduler_type=\"cosine\",  # Use a cosine scheduler for smoother learning rate decay\n    seed=3407,\n    output_dir=\"outputs\",\n    report_to=\"none\"  # Remove additional evaluation and checkpoint parameters\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T02:05:23.168252Z","iopub.execute_input":"2024-11-17T02:05:23.169369Z","iopub.status.idle":"2024-11-17T02:05:23.206414Z","shell.execute_reply.started":"2024-11-17T02:05:23.169315Z","shell.execute_reply":"2024-11-17T02:05:23.205517Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Step 4: Model Fine-Tuning\ntrainer = SFTTrainer(\n    model=model,\n    tokenizer=tokenizer,\n    train_dataset=train_dataset,\n    dataset_text_field=\"text\",\n    max_seq_length=max_seq_length,\n    dataset_num_proc=4,\n    packing=False,  \n    args=training_args\n)\n\ntrainer.train()\n\n# Save model weights after training to avoid loss of progress\nmodel.save_pretrained('outputs_trained')\ntokenizer.save_pretrained('outputs_trained')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T02:05:25.190883Z","iopub.execute_input":"2024-11-17T02:05:25.191648Z","iopub.status.idle":"2024-11-17T05:25:21.008778Z","shell.execute_reply.started":"2024-11-17T02:05:25.191608Z","shell.execute_reply":"2024-11-17T05:25:21.007728Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/30000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afe7be3f59a042719b457047ca331bb1"}},"metadata":{}},{"name":"stderr","text":"max_steps is given, it will override any value given in num_train_epochs\n==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 30,000 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient Accumulation steps = 2\n\\        /    Total batch size = 8 | Total steps = 1,000\n \"-____-\"     Number of trainable parameters = 83,886,080\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 3:19:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.404800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.881900</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.734200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.656900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.664000</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.672900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.657400</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.630200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.629000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.631000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>0.611400</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.639300</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.616600</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.626200</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.649800</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.640300</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.638300</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.615200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.589100</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.585900</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.615900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.627600</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.622700</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.600900</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.627700</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>0.644100</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>0.596800</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>0.598800</td>\n    </tr>\n    <tr>\n      <td>580</td>\n      <td>0.614900</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.578600</td>\n    </tr>\n    <tr>\n      <td>620</td>\n      <td>0.597300</td>\n    </tr>\n    <tr>\n      <td>640</td>\n      <td>0.582300</td>\n    </tr>\n    <tr>\n      <td>660</td>\n      <td>0.624400</td>\n    </tr>\n    <tr>\n      <td>680</td>\n      <td>0.608100</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.630400</td>\n    </tr>\n    <tr>\n      <td>720</td>\n      <td>0.650700</td>\n    </tr>\n    <tr>\n      <td>740</td>\n      <td>0.611300</td>\n    </tr>\n    <tr>\n      <td>760</td>\n      <td>0.579700</td>\n    </tr>\n    <tr>\n      <td>780</td>\n      <td>0.609200</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.590000</td>\n    </tr>\n    <tr>\n      <td>820</td>\n      <td>0.590600</td>\n    </tr>\n    <tr>\n      <td>840</td>\n      <td>0.581000</td>\n    </tr>\n    <tr>\n      <td>860</td>\n      <td>0.569800</td>\n    </tr>\n    <tr>\n      <td>880</td>\n      <td>0.612200</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.611400</td>\n    </tr>\n    <tr>\n      <td>920</td>\n      <td>0.621300</td>\n    </tr>\n    <tr>\n      <td>940</td>\n      <td>0.608100</td>\n    </tr>\n    <tr>\n      <td>960</td>\n      <td>0.609700</td>\n    </tr>\n    <tr>\n      <td>980</td>\n      <td>0.625000</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.596700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"('outputs_trained/tokenizer_config.json',\n 'outputs_trained/special_tokens_map.json',\n 'outputs_trained/tokenizer.json')"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# Step 5: Inference\nFastLanguageModel.for_inference(model)  # Enable fast inference before generating\nis_correct_predictions = []\nfor i in tqdm(range(len(dataset['test']['question'])), desc=\"Inference Progress\"):\n    sample_ques = dataset['test']['question'][i]\n    sample_ans = dataset['test']['answer'][i]\n    sample_solution = dataset['test']['solution'][i]\n\n    # Construct input prompt\n    input_prompt = prompt.format(\n        sample_ques,  # question\n        sample_ans,  # answer\n        sample_solution,  # solution\n        \"\"  # leave output blank for model to generate\n    )\n\n    inputs = tokenizer([input_prompt], return_tensors=\"pt\").to(\"cuda\")\n    input_shape = inputs['input_ids'].shape\n    input_token_len = input_shape[1]\n\n    # Generate model output\n    outputs = model.generate(**inputs, max_new_tokens=10, use_cache=True)\n    response = tokenizer.batch_decode([outputs[0][input_token_len:]], skip_special_tokens=True)[0]\n\n    # Extract True/False from response using regex\n    match = re.search(r'\\b(True|False)\\b', response, re.IGNORECASE)\n    if match:\n        is_correct = match.group(0).lower() == 'true'\n    else:\n        is_correct = False  # Default if no valid output is found\n\n    is_correct_predictions.append(is_correct)\n\n# Save intermediate inference results to avoid loss in case of interruptions\npd.DataFrame({\n    'ID': list(range(len(is_correct_predictions))),\n    'is_correct': is_correct_predictions\n}).to_csv('inference_results_partial.csv', index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 6: Generate CSV for Submission\nsubmission_df = pd.DataFrame({\n    'ID': list(range(len(is_correct_predictions))),\n    'is_correct': is_correct_predictions\n})\n\nsubmission_df.to_csv('submission.csv', index=False)\nprint(\"CSV file 'submission.csv' has been created successfully.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}